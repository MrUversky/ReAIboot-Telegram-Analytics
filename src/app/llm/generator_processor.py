"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPT-4o –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ Reels.
"""

import time
import logging
from typing import Dict, Any, Optional, List

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

from .base_processor import BaseLLMProcessor, ProcessingResult
from ..settings import settings
from ..utils import setup_logger
from ..prompts import prompt_manager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = setup_logger(__name__)


class GeneratorProcessor(BaseLLMProcessor):
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å –ø–æ–º–æ—â—å—é GPT-4o."""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º prompt_manager
        from ..prompts import prompt_manager
        self.prompt_manager = prompt_manager

        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø—Ä–æ–º–ø—Ç–∞
        model_settings = self.prompt_manager.get_model_settings("generate_scenario_system")
        model_name = model_settings.get('model', settings.generator_model)

        super().__init__(
            model_name=model_name,
            api_key=settings.openai_api_key
        )

        if OPENAI_AVAILABLE and self.is_available:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=settings.openai_base_url
            )

    async def process(self, input_data: Dict[str, Any]) -> ProcessingResult:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª—è Reels –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å—Ç–∞ –∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Ä—É–±—Ä–∏–∫–∏.

        Args:
            input_data: –î–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–∞ + –≤—ã–±—Ä–∞–Ω–Ω–∞—è —Ä—É–±—Ä–∏–∫–∞/—Ñ–æ—Ä–º–∞—Ç

        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π
        """
        start_time = time.time()

        if not self.is_available:
            return ProcessingResult(
                success=False,
                error="GPT-4o –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–Ω–µ—Ç API –∫–ª—é—á–∞)",
                processing_time=time.time() - start_time
            )

        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞
            post_text = input_data.get("text", "") or input_data.get("full_text", "") or input_data.get("post_text", "")
            rubric = input_data.get("rubric", {})
            reel_format = input_data.get("reel_format", {})
            analysis = input_data.get("analysis", {})

            # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            logger.info(f"GeneratorProcessor input_data keys: {list(input_data.keys())}")
            logger.info(f"Post text length: {len(post_text)}")
            logger.info(f"Post text preview: {post_text[:200] if post_text else 'EMPTY'}")
            logger.info(f"Rubric: {rubric}")
            logger.info(f"Reel format: {reel_format}")

            if not post_text:
                return ProcessingResult(
                    success=False,
                    error="–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –ø—É—Å—Ç–æ–π",
                    processing_time=time.time() - start_time
                )

            # –ü–æ–ª—É—á–∞–µ–º system –∏ user –ø—Ä–æ–º–ø—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Ñ–æ—Ä–º–∞—Ç—É –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª—è: duration_seconds –∏ duration
            duration = reel_format.get('duration_seconds') or reel_format.get('duration')
            if duration is None:
                duration = 60  # Default to 60 seconds if not found

            system_prompt = self.prompt_manager.get_system_prompt("generate_scenario_system", {
                "duration": duration
            })

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –Ω–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –∏–∑ orchestrator
            user_prompt = self.prompt_manager.get_user_prompt("generate_scenario_system", {
                "post_text": post_text,
                "post_analysis": input_data.get("post_analysis", str(analysis)),
                "rubric_selection_analysis": input_data.get("rubric_selection_analysis", ""),
                "rubric_name": rubric.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–∞'),
                "rubric_description": input_data.get("rubric_description", ""),
                "rubric_examples": input_data.get("rubric_examples", ""),
                "format_name": reel_format.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω'),
                "format_description": input_data.get("format_description", ""),
                "format_duration": input_data.get("format_duration", str(duration)),
                "combination_justification": input_data.get("combination_justification", ""),
                "combination_content_idea": input_data.get("combination_content_idea", ""),
                "duration": duration
            })

            # Debug: –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç—ã
            print(f"üß™ GENERATOR PROMPT - System: {system_prompt}")
            print(f"üß™ GENERATOR PROMPT - User: {user_prompt}")
            print(f"üß™ GENERATOR INPUT DATA: post_text={post_text[:100]}..., rubric={rubric.get('name')}, format={reel_format.get('name')}, duration={duration}")

            # Debug: –ª–æ–≥–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ input_data
            print(f"üß™ GENERATOR DEBUG - rubric_description: '{input_data.get('rubric_description', 'NOT_FOUND')}'")
            print(f"üß™ GENERATOR DEBUG - rubric_examples: '{input_data.get('rubric_examples', 'NOT_FOUND')}'")
            print(f"üß™ GENERATOR DEBUG - format_description: '{input_data.get('format_description', 'NOT_FOUND')}'")
            print(f"üß™ GENERATOR DEBUG - format_duration: '{input_data.get('format_duration', 'NOT_FOUND')}'")
            print(f"üß™ GENERATOR DEBUG - combination_justification: '{input_data.get('combination_justification', 'NOT_FOUND')[:100]}...'")
            print(f"üß™ GENERATOR DEBUG - combination_content_idea: '{input_data.get('combination_content_idea', 'NOT_FOUND')[:100]}...'")

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            success, response, error = await self._make_request_with_retry(
                lambda: self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=3000
                ),
                timeout=90.0  # –¢–∞–π–º–∞—É—Ç 90 —Å–µ–∫—É–Ω–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Å–∞–º—ã–π –¥–æ–ª–≥–∏–π —ç—Ç–∞–ø)
            )

            if not success:
                return ProcessingResult(
                    success=False,
                    error=f"–û—à–∏–±–∫–∞ API: {error}",
                    processing_time=time.time() - start_time
                )

            # –ü–∞—Ä—Å–∏–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            result_text = response.choices[0].message.content
            tokens_used = self._calculate_tokens(user_prompt, result_text)

            # Debug: –ª–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM
            logger.debug(f"üß™ GENERATOR RAW RESPONSE: {result_text}")
            logger.debug(f"üß™ GENERATOR RESPONSE LENGTH: {len(result_text)} chars")
            logger.debug(f"üß™ GENERATOR TOKENS USED: {tokens_used}")

            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –ø–æ —Å—Ö–µ–º–µ
            schema = self.get_stage_schema("generation")
            success, validated_data, validation_error = self.validate_json_response(result_text, schema)

            if success and validated_data:
                return ProcessingResult(
                    success=True,
                    data={
                        **validated_data,
                        "generator_model": self.model_name,
                        "source_post": {
                            "text_preview": post_text[:200],
                            "rubric": rubric.get("name"),
                            "format": reel_format.get("name")
                        }
                    },
                    tokens_used=tokens_used,
                    processing_time=time.time() - start_time,
                    raw_response=result_text
                )
            else:
                logger.warning(f"Generation validation failed: {validation_error}")
                # –ü–æ–ø—ã—Ç–∫–∞ fallback –ø–∞—Ä—Å–∏–Ω–≥–∞
                try:
                    import json
                    import re

                    # –û—á–∏—â–∞–µ–º –æ—Ç markdown –æ–±–µ—Ä—Ç–æ–∫
                    cleaned_text = result_text.strip()
                    if cleaned_text.startswith('```json'):
                        cleaned_text = cleaned_text[7:]
                    if cleaned_text.startswith('```'):
                        cleaned_text = cleaned_text[3:]
                    if cleaned_text.endswith('```'):
                        cleaned_text = cleaned_text[:-3]
                    cleaned_text = cleaned_text.strip()

                    fallback_data = json.loads(cleaned_text)

                    return ProcessingResult(
                        success=True,
                        data={
                            **fallback_data,
                            "generator_model": self.model_name,
                            "source_post": {
                                "text_preview": post_text[:200],
                                "rubric": rubric.get("name"),
                                "format": reel_format.get("name")
                            },
                            "validation_warning": validation_error
                        },
                        tokens_used=tokens_used,
                        processing_time=time.time() - start_time,
                        raw_response=result_text
                    )

                except json.JSONDecodeError:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç GPT-4o: {result_text[:200]}...")

                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
                    if json_match:
                        try:
                            fallback_data = json.loads(json_match.group())
                            return ProcessingResult(
                                success=True,
                                data={
                                    **fallback_data,
                                    "generator_model": self.model_name,
                                    "source_post": {
                                        "text_preview": post_text[:200],
                                        "rubric": rubric.get("name"),
                                        "format": reel_format.get("name")
                                    },
                                    "validation_warning": validation_error
                                },
                                tokens_used=tokens_used,
                                processing_time=time.time() - start_time,
                                raw_response=result_text
                            )
                        except:
                            pass

                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –∫–∞–∫ fallback
                    return ProcessingResult(
                        success=True,
                        data={
                            "raw_scenario": result_text,
                            "generator_model": self.model_name,
                            "parsing_error": "JSON parsing failed",
                            "validation_error": validation_error
                        },
                        tokens_used=tokens_used,
                        processing_time=time.time() - start_time,
                        raw_response=result_text
                    )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ GeneratorProcessor: {e}", exc_info=True)
            return ProcessingResult(
                success=False,
                error=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}",
                processing_time=time.time() - start_time,
                raw_response=None
            )
