"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ—Å—Ç–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPT-4o-mini –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞.
"""

import time
import logging
from typing import Dict, Any, Optional

try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

from .base_processor import BaseLLMProcessor, ProcessingResult
from ..settings import settings
from ..utils import setup_logger
from ..prompts import prompt_manager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = setup_logger(__name__)


class FilterProcessor(BaseLLMProcessor):
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é GPT-4o-mini."""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏."""
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º prompt_manager
        from ..prompts import prompt_manager
        self.prompt_manager = prompt_manager

        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø—Ä–æ–º–ø—Ç–∞
        model_settings = self.prompt_manager.get_model_settings("filter_posts_system")
        model_name = model_settings.get('model', settings.filter_model)

        super().__init__(
            model_name=model_name,
            api_key=settings.openai_api_key
        )

        if OPENAI_AVAILABLE and self.is_available:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=settings.openai_base_url
            )

    async def process(self, input_data: Dict[str, Any]) -> ProcessingResult:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç –ø–æ—Å—Ç –Ω–∞ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å –¥–ª—è –ü–µ—Ä–µ–ø—Ä–æ—à–ò–ò–≤–∫–∞.

        Args:
            input_data: –î–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–∞

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        start_time = time.time()

        if not self.is_available:
            return ProcessingResult(
                success=False,
                error="GPT-4o-mini –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–Ω–µ—Ç API –∫–ª—é—á–∞)",
                processing_time=time.time() - start_time
            )

        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–∞
            post_text = input_data.get("text", "")
            views = input_data.get("views", 0)
            reactions = input_data.get("reactions", 0)
            replies = input_data.get("replies", 0)
            forwards = input_data.get("forwards", 0)
            channel_title = input_data.get("channel_title", "")

            if not post_text:
                return ProcessingResult(
                    success=False,
                    error="–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –ø—É—Å—Ç–æ–π",
                    processing_time=time.time() - start_time
                )

            # –ü–æ–ª—É—á–∞–µ–º system –∏ user –ø—Ä–æ–º–ø—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            system_prompt = self.prompt_manager.get_system_prompt("filter_posts_system", {
                "views": views,
                "reactions": reactions,
                "replies": replies,
                "forwards": forwards
            })

            user_prompt = self.prompt_manager.get_user_prompt("filter_posts_system", {
                "post_text": post_text[:2000],
                "views": views,
                "reactions": reactions,
                "replies": replies,
                "forwards": forwards,
                "channel_title": channel_title
            })

            # Debug: –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç—ã
            logger.debug(f"üß™ FILTER PROMPT - System: {system_prompt}")
            logger.debug(f"üß™ FILTER PROMPT - User: {user_prompt}")
            logger.debug(f"üß™ FILTER INPUT DATA: post_text={post_text[:100]}..., views={views}, reactions={reactions}, forwards={forwards}")

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            success, response, error = await self._make_request_with_retry(
                lambda: self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.3,
                    max_tokens=300
                ),
                timeout=30.0  # –¢–∞–π–º–∞—É—Ç 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞
            )

            if not success:
                return ProcessingResult(
                    success=False,
                    error=f"–û—à–∏–±–∫–∞ API: {error}",
                    processing_time=time.time() - start_time
                )

            # –ü–∞—Ä—Å–∏–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            result_text = response.choices[0].message.content
            tokens_used = self._calculate_tokens(user_prompt, result_text)

            # Debug: –ª–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM
            logger.debug(f"üß™ FILTER RAW RESPONSE: {result_text}")
            logger.debug(f"üß™ FILTER RESPONSE LENGTH: {len(result_text)} chars")
            logger.debug(f"üß™ FILTER TOKENS USED: {tokens_used}")

            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –ø–æ —Å—Ö–µ–º–µ
            schema = self.get_stage_schema("filter")
            success, validated_data, validation_error = self.validate_json_response(result_text, schema)

            if success and validated_data:
                return ProcessingResult(
                    success=True,
                    data={
                        **validated_data,
                        "filter_model": self.model_name
                    },
                    tokens_used=tokens_used,
                    processing_time=time.time() - start_time,
                    raw_response=result_text
                )
            else:
                logger.warning(f"Filter validation failed: {validation_error}")
                # –ü–æ–ø—ã—Ç–∫–∞ fallback –ø–∞—Ä—Å–∏–Ω–≥–∞
                try:
                    import json
                    fallback_data = json.loads(result_text)
                    score = fallback_data.get("score", 0)
                    suitable = fallback_data.get("suitable", score >= 7)
                    reason = fallback_data.get("reason", "–û—Ü–µ–Ω–∫–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞")

                    return ProcessingResult(
                        success=True,
                        data={
                            "score": score,
                            "suitable": suitable,
                            "reason": reason,
                            "filter_model": self.model_name,
                            "validation_warning": validation_error
                        },
                        tokens_used=tokens_used,
                        processing_time=time.time() - start_time,
                        raw_response=result_text
                    )
                except json.JSONDecodeError:
                    return ProcessingResult(
                        success=False,
                        error=f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç–≤–µ—Ç: {result_text}. –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {validation_error}",
                        processing_time=time.time() - start_time,
                        raw_response=result_text
                    )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ FilterProcessor: {e}", exc_info=True)
            return ProcessingResult(
                success=False,
                error=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {str(e)}",
                processing_time=time.time() - start_time,
                raw_response=None
            )
